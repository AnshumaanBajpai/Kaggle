{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Restaurant Photo Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Uncompressing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing the needed modules\n",
    "import numpy as np\n",
    "import ds_utils\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setting up the directories for this project\n",
    "# Data Dir\n",
    "#dataDir = '/c/Users/bajpa/Desktop/Github/KaggleProjects/MassiveData/yelpDS'\n",
    "dataDir = 'C:/Users/bajpa/Desktop/Github/Kaggle/MassiveData/yelpDS'\n",
    "# Analysis Dir\n",
    "anDir = 'C:/Users/bajpa/Desktop/Github/Kaggle/YelpRPC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncompressing all the data files\n",
    "# Extraction function to be run only once\n",
    "def uncompress_data(dirpath):\n",
    "    '''\n",
    "    Extracting the compressed files in dirpath \n",
    "    '''\n",
    "    os.chdir(dirpath)\n",
    "    fileNames = os.listdir(dirpath)\n",
    "#    print fileNames\n",
    "    \n",
    "    for f in fileNames:\n",
    "            ds_utils.uncom_tgz(f)\n",
    "\n",
    "    os.chdir(anDir)\n",
    "    return None\n",
    "\n",
    "#uncompress_data(dataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2) Data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the extracted dataset, we find that we have 6 outputs\n",
    "#### 1) test_photos: A folder containing all the test images\n",
    "#### 2) train_photos: A folder containing all the train images\n",
    "#### 3) sample_submission:  Demo submission file\n",
    "#### 4) train: A csv file to associate charactersitic labels to restaruants\n",
    "#### 5) train_photo_to_biz_ids: A csv file to associate train image ids to specific restaurants\n",
    "#### 6) test_photo_to_biz:  A csv file to associate test image ids to specific restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_photos: \n",
      "files:  237152 \n",
      "size:  7.16280653 gb\n",
      "----------\n",
      "train_photos: \n",
      "files:  234842 \n",
      "size:  7.086361687 gb\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Analyzing the count and size of each of the above\n",
    "def dataset_overview(dirpath):\n",
    "    os.chdir(dirpath)\n",
    "    \n",
    "    # test_photos\n",
    "    path, dirs, files = os.walk(\"./test_photos\").next()\n",
    "    # Excluding the hidden files that were created during extraction process\n",
    "    files = [f for f in files if f[0] != '.']\n",
    "    file_size_bytes = [os.path.getsize(\"./test_photos/\" + f) for f in files]\n",
    "    total_size = sum(file_size_bytes) * 1e-9\n",
    "    print \"test_photos:\", \"\\nfiles: \", len(files), \"\\nsize: \", total_size, \"gb\"\n",
    "    print \"----------\"\n",
    "\n",
    "    # train_photos\n",
    "    path, dirs, files = os.walk(\"./train_photos\").next()\n",
    "    # Excluding the hidden files that were created during extraction process\n",
    "    files = [f for f in files if f[0] != '.']\n",
    "    file_size_bytes = [os.path.getsize(\"./train_photos/\" + f) for f in files]\n",
    "    total_size = sum(file_size_bytes) * 1e-9\n",
    "    print \"train_photos:\", \"\\nfiles: \", len(files), \"\\nsize: \", total_size, \"gb\"\n",
    "    print \"----------\"\n",
    "    \n",
    "    return None\n",
    "    \n",
    "#dataset_overview(dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taking an overview of other csv files with the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "\n",
      "train.csv\n",
      "size in kb\n",
      "28\n",
      "readfile\n",
      "   business_id         labels\n",
      "0         1000  1 2 3 4 5 6 7\n",
      "1         1001        0 1 6 8\n",
      "2          100    1 2 4 5 6 7\n",
      "3         1006      1 2 4 5 6\n",
      "4         1010          0 6 8\n",
      "5          101    1 2 3 4 5 6\n",
      "6         1011        2 3 5 6\n",
      "7         1012      1 2 3 5 6\n",
      "8         1014      1 2 4 5 6\n",
      "9         1015        1 5 6 7\n",
      "----------\n",
      "\n",
      "train_photo_to_biz_ids.csv\n",
      "size in kb\n",
      "2698\n",
      "readfile\n",
      "   photo_id  business_id\n",
      "0    204149         3034\n",
      "1     52779         2805\n",
      "2    278973          485\n",
      "3    195284          485\n",
      "4     19992          485\n",
      "5     80748          485\n",
      "6    444996         1783\n",
      "7    200285           35\n",
      "8     90572           35\n",
      "9     27565         1313\n",
      "----------\n",
      "\n",
      "test_photo_to_biz.csv\n",
      "size in kb\n",
      "15193\n",
      "readfile\n",
      "   photo_id business_id\n",
      "0    317818       003sg\n",
      "1     30679       003sg\n",
      "2    455084       003sg\n",
      "3    371381       003sg\n",
      "4     86224       003sg\n",
      "5     36076       003sg\n",
      "6     46999       003sg\n",
      "7     74896       003sg\n",
      "8    169399       003sg\n",
      "9    110581       003sg\n",
      "----------\n",
      "\n",
      "sample_submission.csv\n",
      "size in kb\n",
      "120\n",
      "readfile\n",
      "  business_id labels\n",
      "0       003sg  1 2 3\n",
      "1       00er5  1 2 3\n",
      "2       00kad  1 2 3\n",
      "3       00mc6  1 2 3\n",
      "4       00q7x  1 2 3\n",
      "5       00v0t  1 2 3\n",
      "6       00y7p  1 2 3\n",
      "7       019fg  1 2 3\n",
      "8       019r1  1 2 3\n",
      "9       01i5j  1 2 3\n"
     ]
    }
   ],
   "source": [
    "# CSV file list\n",
    "def csv_overview(dirpath):\n",
    "    os.chdir(dirpath)\n",
    "    \n",
    "    # files to be analysed\n",
    "    csv_list = ['train.csv', 'train_photo_to_biz_ids.csv',\n",
    "                'test_photo_to_biz.csv', 'sample_submission.csv']\n",
    "    \n",
    "    # Analyzing the files\n",
    "    for filename in csv_list:\n",
    "        print \"----------\\n\"\n",
    "        f_overview = ds_utils.csv_overview(filename)\n",
    "        for keyname in f_overview:\n",
    "            print keyname\n",
    "            print f_overview[keyname]\n",
    "\n",
    "    return None\n",
    "\n",
    "#csv_overview(dataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So we have a large number of image files in both test and training data set. Each of them has an image id. 1) 'train.csv': business ids and a certain number of tags related to it which signify the characteristics of the restaurant. 2) 'train_photo_to_biz_ids.csv': each photo is related to a particular business id. 3) 'test_photo_to_biz.csv': business ids for each image in the test_photos. 4) 'sample_submission.csv': We need to associate labels to each business_id.\n",
    "#### The csv files are not too big and can easily be loaded into the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3) Splitting the training data into train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading the train.csv file\n",
    "def train_valid(dirpath):\n",
    "    # moving to the data folder\n",
    "    os.chdir(dirpath)\n",
    "    \n",
    "    # reading business ids\n",
    "    trainBizIdsLabel = pd.read_csv(\"train.csv\", sep=',', skipinitialspace=True,\n",
    "                            skip_blank_lines=True)\n",
    "    \n",
    "    # Shuffle the entire dataframe and then split it into training and validation set\n",
    "    trainBizIdsLabelShuffle = trainBizIdsLabel.reindex(np.random.permutation(trainBizIdsLabel.index))\n",
    "    # We split the ids into a train:validate::0.75:0.25\n",
    "    split_at = int(0.75 * trainBizIdsLabelShuffle.shape[0])\n",
    "\n",
    "    # Training set data\n",
    "    trSetBizIds = trainBizIdsLabelShuffle.iloc[:split_at, :]\n",
    "    valSetBizIds = trainBizIdsLabelShuffle.iloc[split_at:, :]\n",
    "#    print trSetBizIds.shape\n",
    "#    print valSetBizIds.shape\n",
    "    \n",
    "    # reading biz_id to image id\n",
    "    trainImgBizId = pd.read_csv(\"train_photo_to_biz_ids.csv\", sep=',', skipinitialspace=True,\n",
    "                            skip_blank_lines=True)\n",
    "    \n",
    "#    print trainImgBizId.shape\n",
    "    # Dictionary that will have business id as key and and values will be dictionary of labels imageIds\n",
    "    trainDic = {}\n",
    "    for ir in trSetBizIds.itertuples():\n",
    "        trainDic[str(ir[1])] = {'label':ir[2],\n",
    "                                'img':trainImgBizId[trainImgBizId['business_id'] == ir[1]].iloc[:, 0].values}\n",
    "\n",
    "    valDic = {}\n",
    "    for ir in valSetBizIds.itertuples():\n",
    "        valDic[str(ir[1])] = {'label':ir[2],\n",
    "                              'img':trainImgBizId[trainImgBizId['business_id'] == ir[1]].iloc[:, 0].values}\n",
    "\n",
    "#    print len(trainDic.keys())\n",
    "#    print len(valDic.keys())\n",
    "    # Saving the relevant dictionaries for future use.\n",
    "    train_set = open('trainDic.pkl', 'wb')\n",
    "    pickle.dump(trainDic, train_set)\n",
    "    train_set.close()\n",
    "\n",
    "    val_set = open('valDic.pkl', 'wb')\n",
    "    pickle.dump(valDic, val_set)\n",
    "    val_set.close()\n",
    "\n",
    "    return None\n",
    "\n",
    "#train_valid(dataDir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
